{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30665,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Fine Tuning Mistral-7B","metadata":{}},{"cell_type":"markdown","source":"### Road Map;\n* Loading Tokenizer of Mistral\n* Loading Base Model of Mistral\n* Size Reduction with Quantization\n* Loading Dataset\n* Training with PEFT(Parameter Efficient Fine Tuning) Technique\n* Inference","metadata":{}},{"cell_type":"code","source":"!pip install -q -U transformers bitsandbytes peft datasets trl","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Loading Tokenizer","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer\n\nbase_model = \"mistralai/Mistral-7B-v0.1\"\n\ntokenizer = AutoTokenizer.from_pretrained(base_model,\n                                         padding_side=\"right\",\n                                         add_eos_token=True)\n\ntokenizer.pad_token = tokenizer.eos_token","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer.add_bos_token, tokenizer.add_eos_token","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Loading the Model with Quantization","metadata":{}},{"cell_type":"code","source":"from transformers import BitsAndBytesConfig\nimport torch\n\nbnb_config = BitsAndBytesConfig(load_in_4bit=True,\n                                bnb_4bit_quant_type=\"nf4\",\n                                bnb_4bit_use_double_quant=False,\n                                bnb_4bit_compute_dtype=torch.bfloat16)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM\n\nmodel = AutoModelForCausalLM.from_pretrained(base_model,\n                                            quantization_config=bnb_config,\n                                            torch_dtype=torch.bfloat16,\n                                            device_map=\"auto\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Loading the Dataset","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset\n\ndataset_name = \"databricks/databricks-dolly-15k\"\n\ntrain_dataset = load_dataset(dataset_name, split=\"train[0:800]\")\neval_dataset = load_dataset(dataset_name, split=\"train[800:1000]\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset.to_pandas().dtypes","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset.to_pandas().value_counts(\"category\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Setting Prompt Format","metadata":{}},{"cell_type":"code","source":"def generate_prompt(sample):\n    \n    full_prompt = f\"\"\"<s>[INST]{sample['instruction']}\n    {f\"Here is some context: {sample['context']}\" if len(sample['context']) > 0 else None}\n    [/INST] {sample['response']}</s>\n    \"\"\"\n    \n    return {\"text\": full_prompt}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"generate_prompt(train_dataset[0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"generated_train_dataset = train_dataset.map(generate_prompt,\n                                           remove_columns=list(train_dataset.features))\n\ngenerated_val_dataset = eval_dataset.map(generate_prompt,\n                                        remove_columns=list(train_dataset.features))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"generated_train_dataset[5]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"generated_train_dataset","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from peft import prepare_model_for_kbit_training\n\nmodel.gradient_checkpointing_enable()\n\nmodel = prepare_model_for_kbit_training(model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def print_trainable_parameters(model):\n    trainable_params = 0\n    all_param = 0\n    for _, param in model.named_parameters():\n        all_param += param.numel()\n        if param.requires_grad:\n            trainable_params += param.numel()\n    print(f\"trainable params: {trainable_params} || all params: {all_param} || trainable %{100*trainable_params/all_param}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### LoRA Configuraion","metadata":{}},{"cell_type":"code","source":"from peft import LoraConfig, get_peft_model\n\nlora_config = LoraConfig(r=8,# egitilecek parametre sayisi 8-128\n                         lora_alpha=16,# ogrenilecek agirliklar icin olcekleme parametresidir\n                         target_modules=[\n                             \"q_proj\",\n                             \"k_proj\",\n                             \"v_proj\",\n                             \"o_proj\",\n                             \"gate_proj\",\n                             \"up_proj\",\n                             \"down_proj\",\n                             \"lm_head\",\n                         ],\n                         bias=\"none\",\n                         lora_dropout=0.05,\n                         task_type=\"CAUSAL_LM\",\n                        )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = get_peft_model(model, lora_config)\n\nprint_trainable_parameters(model) # ne kadarlik kisim train edilecek bakalim","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(model) # modele eklenen LoRA katmanlari","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Setting Hyperparameters","metadata":{}},{"cell_type":"code","source":"from huggingface_hub import notebook_login\n\nnotebook_login()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import TrainingArguments\n\ntraining_arguments = TrainingArguments(output_dir=\"./results\",\n                                      num_train_epochs=5,\n                                      per_device_train_batch_size=4,\n                                      gradient_accumulation_steps=1,\n                                      optim=\"paged_adamw_32bit\",\n                                      save_strategy=\"steps\",\n                                      save_steps=25,\n                                      learning_rate=2e-4,\n                                      weight_decay=0.001,\n                                      max_steps=100,\n                                      evaluation_strategy=\"steps\",\n                                      eval_steps=25,\n                                      do_eval=True,\n                                      report_to=\"none\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Training the Model","metadata":{}},{"cell_type":"code","source":"from trl import SFTTrainer\n\ntrainer = SFTTrainer(model=model,\n                    tokenizer=tokenizer,\n                    args=training_arguments,\n                    train_dataset=generated_train_dataset,\n                    eval_dataset=generated_val_dataset,\n                    peft_config=lora_config,\n                    dataset_text_field=\"text\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.config.use_cache = False","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer.train()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"my_fine_tuned_model = \"mistral-7b-dolly-fine-tuned\"\ntrainer.model.push_to_hub(my_fine_tuned_model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}